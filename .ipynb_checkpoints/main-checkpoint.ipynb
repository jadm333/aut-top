{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import preprocessor as p\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jadm333/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#%% Leer archivos y hacer una primera limpieza\n",
    "\n",
    "path =\"Archivos_csv/\"\n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI,p.OPT.RESERVED,p.OPT.SMILEY,p.OPT.NUMBER)\n",
    "for file_ in allFiles:\n",
    "    df =  pd.read_csv(file_,header=0, parse_dates=True, infer_datetime_format=True, index_col=0)\n",
    "    df['screen_name'] = os.path.splitext(os.path.basename(file_))[0]\n",
    "    df = df.loc[df['RT_temp'] == 0]\n",
    "    del df['id_tweet']\n",
    "    del df['id_twitter']\n",
    "    del df['created_at']\n",
    "    del df['in_reply_to_user_id']\n",
    "    del df['in_reply_to_status_id']\n",
    "    del df['in_reply_to_screen_name']\n",
    "    del df['retweet_count']\n",
    "    del df['favorite_count']\n",
    "    del df['longitude']\n",
    "    del df['latitude']\n",
    "    del df['retweeted']\n",
    "    del df['creation_date']\n",
    "    del df['modification_date']\n",
    "    del df['RT_temp']\n",
    "    del df['is_retweeted']\n",
    "    df = df.loc[df['created_at_datetime'] > \"2017-07-04\"]\n",
    "    df['text'] = df['text'].apply(p.clean)\n",
    "    df['text'].replace('', np.nan, inplace=True)\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    df = df.drop_duplicates(subset = \"text\", keep='last')\n",
    "    list_.append(df)\n",
    "df = pd.concat(list_,ignore_index=True)\n",
    "del allFiles\n",
    "del file_\n",
    "del frame\n",
    "del list_\n",
    "del path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#%%\n",
    "import spacy\n",
    "nlp = spacy.load('es')\n",
    "#%%\n",
    "docs = list(df['text'])\n",
    "#%%\n",
    "#%%time\n",
    "processed_docs = []    \n",
    "for doc in nlp.pipe(docs, n_threads=4, batch_size=100):\n",
    "\n",
    "    ents = doc.ents  \n",
    "\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "    \n",
    "    processed_docs.append(doc)\n",
    "\n",
    "#%%\n",
    "docs = processed_docs\n",
    "del processed_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jadm333/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:274: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#%%\n",
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            docs[idx].append(token)\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "max_freq = 0.5\n",
    "min_wordcount = 20\n",
    "dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "\n",
    "_ = dictionary[0] \n",
    "#%%\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jadm333/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:274: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "#%% Borrar tweets vacios y actualizar corpus\n",
    "id_borrar = [i for i in range(0,len(corpus)) if len(corpus[i]) == 0]\n",
    "df = df.drop(df.index[id_borrar])\n",
    "df = df.reset_index(drop=True)\n",
    "docs = list(df['text'])\n",
    "processed_docs = []    \n",
    "for doc in nlp.pipe(docs, n_threads=4, batch_size=100):\n",
    "    ents = doc.ents\n",
    "\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "    processed_docs.append(doc)\n",
    "docs = processed_docs\n",
    "del processed_docs\n",
    "from gensim.models import Phrases\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            docs[idx].append(token)\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "max_freq = 0.5\n",
    "min_wordcount = 20\n",
    "dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "_ = dictionary[0]\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "#%%Crear author2doc\n",
    "author2doc = {}\n",
    "df.text.unique()\n",
    "for aut in df.screen_name.unique():\n",
    "    author2doc[aut] = []\n",
    "    \n",
    "for index, row in df.iterrows():\n",
    "    author2doc[row['screen_name']].append(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de autores: 136\n",
      "# tokens unicos: 3170\n",
      "# de documentos: 41773\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('# de autores: %d' % len(author2doc))\n",
    "print('# tokens unicos: %d' % len(dictionary))\n",
    "print('# de documentos: %d' % len(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 26s, sys: 168 ms, total: 22min 26s\n",
      "Wall time: 22min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import AuthorTopicModel\n",
    "model = AuthorTopicModel(corpus=corpus, num_topics=100, id2word=dictionary.id2token, author2doc=author2doc, chunksize=1000, passes=25, eval_every=1, iterations=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('modelo5/model.atmodel')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
